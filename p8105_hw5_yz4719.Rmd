---
title: "p8105_hw5_yz4719"
author: "Yuxin Zhang"
date: "2023-11-11"
output: github_document
---


```{r setup, include=FALSE}
library(tidyverse)
library(p8105.datasets)
library(dplyr)
library(purrr)
library(ggplot2)
```

## Problem 1

For this problem, we are interested in data gathered and made public by _The Washington Post_ on homicides in 50 large U.S. cities. The code chunk below imports and cleans the data.

```{r}
homicide_df = 
  read_csv("data/homicide-data.csv", na = c("", "NA", "Unknown")) |>
  mutate(
    city_state = str_c(city, state, sep = ", "),
    resolution = case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest"        ~ "unsolved",
      disposition == "Closed by arrest"      ~ "solved"
    )
  ) |> 
  filter(city_state != "Tulsa, AL") 
```

The resulting dataframe has `r nrow(homicide_df)` entries, on variables that include the victim name, race, age, and sex; the date the homicide was reported; and the location of the homicide. In cleaning, I created a `city_state` variable that includes both city and state, and a `resolution` variable to indicate whether the case was closed by arrest. I also excluded one entry in Tulsa, AL, which is not a major US city and is most likely a data entry error. 

In the next code chunk, I group within cities and summarize to produce the total number of homicides and the number that are solved. 

```{r}
city_homicide_df = 
  homicide_df |> 
  select(city_state, disposition, resolution) |> 
  group_by(city_state) |> 
  summarize(
    hom_total = n(),
    hom_unsolved = sum(resolution == "unsolved"))
```

Focusing only on Baltimore, MD, I can use the `prop.test` and `broom::tidy` functions to obtain an estimate and CI of the proportion of unsolved homicides in that city. The table below shows those values.

```{r}
bmore_test = 
  prop.test(
    x = filter(city_homicide_df, city_state == "Baltimore, MD") |> pull(hom_unsolved),
    n = filter(city_homicide_df, city_state == "Baltimore, MD") |> pull(hom_total)) 

broom::tidy(bmore_test) |> 
  knitr::kable(digits = 3)
```

Building on this code, I can use functions in the `purrr` package to obtain estimates and CIs for the proportion of unsolved homicides in each city in my dataset. The code below implements this analysis. 

```{r}
test_results = 
  city_homicide_df |> 
  mutate(
    prop_tests = map2(hom_unsolved, hom_total, \(x, y) prop.test(x = x, n = y)),
    tidy_tests = map(prop_tests, broom::tidy)) |> 
  select(-prop_tests) |> 
  unnest(tidy_tests) |> 
  select(city_state, estimate, conf.low, conf.high) |> 
  mutate(city_state = fct_reorder(city_state, estimate))
```

Finally, I make a plot showing the estimate (and CI) of the proportion of unsolved homicides in each city.

```{r}
test_results |> 
  mutate(city_state = fct_reorder(city_state, estimate)) |> 
  ggplot(aes(x = city_state, y = estimate)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

This figure suggests a very wide range in the rate at which homicides are solved -- Chicago is noticeably high and, given the narrowness of the CI, likely is the location of many homicides. 


## Problem 2

Use this zip file which contains data from a longitudinal study that included a control arm and an experimental arm. Data for each participant is included in a separate file, and file names include the subject ID and arm.

Create a tidy dataframe containing data from all participants, including the subject ID, arm, and observations over time.


*Start with a dataframe containing all file names; the list.files function will help,Iterate over file names and read in data for each subject using purrr::map and saving the result as a new variable in the dataframe*
```{r}
q2_df = 
  tibble(
    files = list.files("data/q2data/"),
    path = str_c("data/q2data/", files)
  ) |>
  mutate(data = map(path, read_csv)) |>
  unnest()
```

*Tidy the result; manipulate file names to include control arm and subject ID, make sure weekly observations are “tidy”, and do any other tidying that’s necessary*

```{r}
q2_tidy_df = 
  q2_df|>
  janitor::clean_names() |>
  separate(files, into = c("arm", "subject_id"), sep = "_") |>
  mutate(
    arm = case_match(
      arm, 
      "con" ~ "control", 
      "exp" ~ "experiment")) |>
  mutate(subject_id = str_replace(subject_id, ".csv", "")) |>
  pivot_longer(week_1:week_8, 
               names_to = "week",
               names_prefix = "week_",
               values_to = "observation") |>
  select(arm, subject_id, week, observation)

q2_tidy_df
```

*Make a spaghetti plot showing observations on each subject over time, and comment on differences between groups*

```{r}
q2_tidy_df|>
  ggplot(aes(x = week, y = observation, group = subject_id, color = subject_id)) +
  geom_point()+
  geom_line() +
  facet_grid(.~arm) +
  labs(x = "weeks",
       y = "observations",
       title = "Observations on each subject over time") + theme_minimal()
```

**The spaghetti plot shows that the observations on each subject in control group fluctuates but remain stable, while those in experimental group are increasing over time. And compared to control group, the mean values of observations of each subject are higher in experimental group.**

## Question 3


*First set the following design elements:Fix n=30, Fix σ=5*

*Set μ=0. Generate 5000 datasets from the model: x∼Normal[μ,σ]*

*For each dataset, save μ^ and the p-value arising from a test of H:μ=0 using α=0.05.*

```{r}
set.seed(1)
sim_norm =
  function(n=30, mu, sd=5){
    x = rnorm(n, mean=mu, sd)
    test_result = t.test(x)|>
      broom::tidy() |>
      select(estimate, p.value)
    }
```

```{r }
sim_mu_0 =
  rerun(5000, sim_norm(mu = 0)) |> 
  bind_rows() |> 
  select(estimate, p.value)
```

*repeat the above for μ=1,2,3,4,5,6*

```{r}
sim_mu = 
 tibble(mu = c(0, 1, 2, 3, 4, 5, 6)
        ) |> 
  mutate(
    output_list = map(.x = mu, ~ rerun(5000, sim_norm(mu = .x, sd=5))),
    estimate_df = map(output_list, bind_rows)
    )|>  
  unnest(estimate_df) |> 
  select(-output_list)
```

*Make a plot showing the proportion of times the null was rejected (the power of the test) on the y axis and the true value of μ on the x axis. Describe the association between effect size and power.*

```{r}
plot_power_df = 
  sim_mu |>
  group_by(mu) |>
  summarize(reject = sum(p.value < 0.05)/5000) |>
  ggplot(aes(x = mu, y = reject)) +
  geom_point() +
  geom_line() +
  labs(
    title = "Effect size and power",
    x = "True value of mean",
    y = "Power") +
    theme_minimal()

plot_power_df
```

**The plot indicates a positive linear correlation between the power of the test and the true value of μ, showing that as μ increases from 1 to 6, the proportion of times the null hypothesis is rejected also increases. This trend highlights that larger effect sizes, which are easier to detect, are associated with increased power, ultimately leading to a higher likelihood of detecting statistically significant differences as the effect size grows.**

*Make a plot showing the average estimate of μ^ on the y axis and the true value of μ on the x axis. Make a second plot (or overlay on the first) the average estimate of μ^ only in samples for which the null was rejected on the y axis and the true value of μ on the x axis. Is the sample average of μ^ across tests for which the null is rejected approximately equal to the true value of μ? Why or why not?*

```{r}
plot_avg_mean_df= 
  sim_mu |> 
  group_by(mu) |> 
  mutate(avg_mean = mean(estimate)) |> 
  ggplot(aes(x = mu, y = avg_mean ))+
  geom_point() +
  geom_line() +
  labs(
    title = "Average estimate of mean and true mean",
    x = "True value of mean",
    y = "Average estimate"
  ) +
  theme_minimal()

plot_avg_mean_df
```

**This shows that the average estimated mean closely aligns with the true mean, adhering to a relationship best represented by the linear equation y = x.**

*Make a second plot (or overlay on the first) the average estimate of μ^ only in samples for which the null was rejected on the y axis and the true value of μ on the x axis. Is the sample average of μ^ across tests for which the null is rejected approximately equal to the true value of μ ? Why or why not?*
```{r}
plot_null_reject_df =  
  sim_mu |> 
  filter(p.value < 0.05) |> 
  group_by(mu) |> 
  mutate(avg_mean = mean(estimate)) |> 
  ggplot(aes(x = mu, y = avg_mean ))+
  geom_point()+
  geom_line() +
  labs(
    title = "Average estimate of mean and true mean in null rejected samples",
    x = "True value of mean",
    y = "Average estimate mean"
  ) + 
  theme_minimal() 

plot_null_reject_df
```

```{r}
null_reject_data <- sim_mu |>
  filter(p.value < 0.05) |>
  group_by(mu) |>
  summarise(avg_mean = mean(estimate)) |>
  mutate(plot_type = "null_reject")

avg_mean_data <- sim_mu |>
  group_by(mu) |>
  summarise(avg_mean = mean(estimate)) |>
  mutate(plot_type = "avg_mean")

# Combine the data
combined_data <- rbind(null_reject_data, avg_mean_data)

combined_plot <- ggplot(combined_data, aes(x = mu, y = avg_mean, color = plot_type)) +
  geom_point() +
  geom_line(aes(group = plot_type)) +
  labs(
    title = "Combined Plot",
    x = "True value of mean",
    y = "Average estimate of mean"
  ) +
  theme_minimal() +
  scale_color_manual(values = c("null_reject" = "blue", "avg_mean" = "red"))

print(combined_plot)

```

**Across various tests, the sample average of μ closely approximates the true value of μ, especially when the true value is equal to or greater than 4, reflecting the influence of larger effect sizes and higher test power. Additionally, for tests where the null hypothesis is not rejected when the true mean is not zero, the estimated effect size tends to be lower, indicating that higher sample sizes yield more accurate estimates of the true effect size.**